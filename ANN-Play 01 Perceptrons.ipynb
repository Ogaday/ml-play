{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network Play\n",
    "\n",
    "An exploration of artificial neural networks. I already have an understanding on artificial neural networks from my final year at university, but I think that understanding is quite theoretical and shallow. I would like to play with neural networks so that I become familiar with their intricacies and gotchas. The purpose then of this document is to start from the very bottom and work my way up, developing the components of and then a whole artificial neural network from scratch before moving onto using established libraries (PyBrain, scikit-learn, etc.) and employing optimisations, such as using Theano.\n",
    "\n",
    "There are a couple of tutorials, articles and such which I might be following closely, along with other more rigorous sources. They are:\n",
    " - https://datajobs.com/data-science-repo/Neural-Net-%5BCarlos-Gershenson%5D.pdf\n",
    " - http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    " - http://andrew.gibiansky.com/blog/machine-learning/coding-intro-to-nns/\n",
    " - http://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    " \n",
    "I will endeavour to cite other sources I use as I go.\n",
    "\n",
    "## Basic Understanding\n",
    "\n",
    "The anatomy of a neural network seems clear: it consists of neurons ands connections, corresponding to nodes and directed, weighted edges in the terminology of graphs. I believe that artifificial neural networks will always be able to be represented as directed, weighted, non cyclic-graphs. The network will consist of one input layer (one node for each input), one output layer (one node for each output) and any number of \"hidden\" layers, layers which do not recieve direct inputs or produce direct outputs. Each neuron will have inputs and one output. An \"activation function\" will produce a result, analogous to a real neuron firing, based upon the inputs multiplied by the weights. The weights characterise the network, and the power of neural networks comes in learning the weights, so that the final outputs match specific targets. And that, in a nutshell, is the essense of a neural network (without touching on backpropogation at all)!\n",
    "\n",
    "## Play\n",
    "\n",
    "The initial exploration will be based on Carlos Gershenson's pdf on the topic of artificial neural networks. It's an ab initio introduction, and will introduce ANNs in simple terms that I can then emulate in code.\n",
    "\n",
    "### Perceptrons\n",
    "\n",
    "Perceptrons were the first form of artificial neural network to be considered. They consist of a single layer, with inputs and a single output. They can be used as a binary classifier, for instance. Let's see if I can build one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "Dand = np.array([[1,1,1],[1,0,0],[0,1,0],[0,0,0]])\n",
    "Dor =  np.array([[1,1,1],[1,0,1],[0,1,1],[0,0,0]])\n",
    "# Dnand, Dnor, Dxor, Dxand, etc...\n",
    "\n",
    "# Activation function\n",
    "def heaviside(i):\n",
    "    return 1 if i > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f572ecd7c18>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAAHpCAYAAABXxZ8aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f572ecccc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's look at our data\n",
    "I = Dand[:,-1] == 1\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(Dand[I][:,0], Dand[I][:,1], 'ro', label = \"t = 1\")\n",
    "plt.plot(Dand[I==False][:,0], Dand[I==False][:,1], 'bo', label = \"t = 0\")\n",
    "ax = plt.axis([-0.2, 1.2, -0.1, 1.2])\n",
    "plt.legend()\n",
    "plt.title(\"AND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5488135   0.71518937  0.60276338]\n",
      "[ 1.  1.  1.] 1\n",
      "[ 1.  1.  0.] 0\n",
      "[ 1.  0.  1.] 0\n",
      "[ 1.  0.  0.] 0\n"
     ]
    }
   ],
   "source": [
    "# Logical perceptron.\n",
    "# Three inputs, one out.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate our weights\n",
    "w = np.random.rand(3)\n",
    "print(w)\n",
    "\n",
    "# Look at ins and outs\n",
    "x = np.ones(3)\n",
    "for i in range(Dand.shape[0]):\n",
    "    x[1:] = Dand[i,:2]\n",
    "    print(x, Dand[i,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out is 1 target is 1\n",
      "Out is 1 target is 0\n",
      "Out is 1 target is 0\n",
      "Out is 1 target is 0\n"
     ]
    }
   ],
   "source": [
    "# Activate the neural network!\n",
    "for i in range(Dand.shape[0]):\n",
    "    x[1:] = Dand[i,:2]\n",
    "    out = heaviside(np.sum(np.dot(w, x)))\n",
    "    print(\"Out is \" + str(out) + \" target is \" + str (Dand[i, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this isn't great, but we have out perceptron firing! At this stage, this could be cleaned up and then used in training to change the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training the weights / implementing the algorithm found on wikipedia here:\n",
    "# https://en.wikipedia.org/wiki/Perceptron\n",
    "\n",
    "# Definitions:\n",
    "# ouput of the perceptron\n",
    "def f(z):\n",
    "    return heaviside(np.sum(z))\n",
    "\n",
    "\n",
    "\n",
    "# Data\n",
    "D = np.ones((4,4))\n",
    "D[:,1:] = Dand\n",
    "\n",
    "# Vector of weights, zero or small random number.\n",
    "w = np.zeros(3)\n",
    "\n",
    "# Learning rate 0 < alpha < 1. Can't be too high to will oscillate around solution.\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.]\n",
      " [ 1.  1.  0.  0.]\n",
      " [ 1.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Algorithm\n",
    "\n",
    "for t in range(100):\n",
    "    for row in D:\n",
    "        # calculate out\n",
    "        out = f(np.dot(row[:3], w))\n",
    "        # update weights\n",
    "        for i, wi in enumerate(w):\n",
    "            w[i] = wi + alpha*(row[-1]- out)*row[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [-0.2  0.1  0.2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained weights:\", w)\n",
    "\n",
    "# Does it work?\n",
    "\n",
    "for i, row in enumerate(D):\n",
    "    assert(f(np.dot(row[:3], w)) == row[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, I don't quite believe it, but I think it works! I managed to train a perceptron to identify the logical AND relation. Can I generalise and tackle the other logical relations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def heaviside(i):\n",
    "    return 1 if i > 0 else 0\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, ni, bias = False, weights = None, func = heaviside):\n",
    "        \"\"\"\n",
    "        Create a perceptron instance.\n",
    "        \"\"\"\n",
    "        # Number of inputs\n",
    "        self.ni = ni\n",
    "        \n",
    "        # Does the input vector contain a term for the bias (x_0) ?\n",
    "        self.bias = bias\n",
    "        \n",
    "        if weights:\n",
    "            self.weights = list(weights)\n",
    "            if bias:\n",
    "                assert(ni == len(weights))\n",
    "            else:\n",
    "                assert(ni+1 == len(weights))\n",
    "        else:\n",
    "            self.weights = [0]*ni\n",
    "            if not self.bias:\n",
    "                self.weights += [0]\n",
    "            \n",
    "        self.func = func\n",
    "            \n",
    "    def activate(self, in_vec):\n",
    "        in_vec = list(in_vec)\n",
    "        if not self.bias:\n",
    "            in_vec = [1]+in_vec\n",
    "        assert(len(in_vec)==len(self.weights))\n",
    "        return self.func(sum([i*w for i, w in zip(in_vec, self.weights)]))\n",
    "    \n",
    "    def train(Xtr, alpha, iterations):\n",
    "        pass\n",
    "        # Assert size of Xtr is correct\n",
    "        # have a default alpha size\n",
    "        # have a default number of iterations\n",
    "        # perhaps have some arguments about other termination criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [0.5488135039273248, 0.7151893663724195, 0.6027633760716439]\n",
      "\n",
      "Features and targets:\n",
      " [[1 1 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]] \n",
      "\n",
      "Out is 1 target is 1\n",
      "Out is 1 target is 0\n",
      "Out is 1 target is 0\n",
      "Out is 1 target is 0\n"
     ]
    }
   ],
   "source": [
    "# See if we can reproduce the same activation result as earlier.\n",
    "np.random.seed(0)\n",
    "weights = [np.random.rand() for i in range(3)]\n",
    "\n",
    "print(\"Weights:\\n\", weights)\n",
    "print(\"\\nFeatures and targets:\\n\",Dand,\"\\n\")\n",
    "\n",
    "pec = Perceptron(2, weights = weights)\n",
    "\n",
    "for i in range(Dand.shape[0]):\n",
    "    out = pec.activate(Dand[i,:2])\n",
    "    print(\"Out is \" + str(out) + \" target is \" + str (Dand[i, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same result for activating the perceptron as above. Nice. It's a poor test though and I'm going to have to do a few more in a test file. For now though, onto the training algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [0, 0, 0]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [0.10000000000000001, 0.10000000000000001, 0.10000000000000001]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [0.0, 0.0, 0.10000000000000001]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [0.0, 0.0, 0.10000000000000001]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [0.0, 0.0, 0.10000000000000001]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [0.0, 0.0, 0.10000000000000001]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.10000000000000001, -0.10000000000000001, 0.10000000000000001]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.10000000000000001, -0.10000000000000001, 0.10000000000000001]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.10000000000000001, -0.10000000000000001, 0.10000000000000001]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [0.0, 0.0, 0.20000000000000001]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.10000000000000001, -0.10000000000000001, 0.20000000000000001]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.10000000000000001, -0.10000000000000001, 0.20000000000000001]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.10000000000000001, -0.10000000000000001, 0.20000000000000001]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [0.0, 0.0, 0.30000000000000004]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.10000000000000001, -0.10000000000000001, 0.30000000000000004]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.10000000000000001, -0.10000000000000001, 0.30000000000000004]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.10000000000000001, -0.10000000000000001, 0.30000000000000004]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.10000000000000001, -0.10000000000000001, 0.30000000000000004]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.20000000000000001, -0.20000000000000001, 0.30000000000000004]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.20000000000000001, -0.20000000000000001, 0.30000000000000004]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.20000000000000001, -0.20000000000000001, 0.30000000000000004]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.10000000000000001, -0.10000000000000001, 0.40000000000000002]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.20000000000000001, -0.20000000000000001, 0.40000000000000002]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.20000000000000001, -0.20000000000000001, 0.40000000000000002]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.20000000000000001, -0.20000000000000001, 0.40000000000000002]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.10000000000000001, -0.10000000000000001, 0.5]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.20000000000000001, -0.20000000000000001, 0.5]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.20000000000000001, -0.20000000000000001, 0.5]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.20000000000000001, -0.20000000000000001, 0.5]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.20000000000000001, -0.20000000000000001, 0.5]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.30000000000000004, -0.30000000000000004, 0.5]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.30000000000000004, -0.30000000000000004, 0.5]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.30000000000000004, -0.30000000000000004, 0.5]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.20000000000000004, -0.20000000000000004, 0.59999999999999998]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.30000000000000004, -0.30000000000000004, 0.59999999999999998]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.30000000000000004, -0.30000000000000004, 0.59999999999999998]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.30000000000000004, -0.30000000000000004, 0.59999999999999998]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.20000000000000004, -0.20000000000000004, 0.69999999999999996]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.30000000000000004, -0.30000000000000004, 0.69999999999999996]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.30000000000000004, -0.30000000000000004, 0.69999999999999996]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.30000000000000004, -0.30000000000000004, 0.69999999999999996]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.30000000000000004, -0.30000000000000004, 0.69999999999999996]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.40000000000000002, -0.40000000000000002, 0.69999999999999996]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.40000000000000002, -0.40000000000000002, 0.69999999999999996]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.40000000000000002, -0.40000000000000002, 0.69999999999999996]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.30000000000000004, -0.30000000000000004, 0.79999999999999993]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.40000000000000002, -0.40000000000000002, 0.79999999999999993]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.40000000000000002, -0.40000000000000002, 0.79999999999999993]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.40000000000000002, -0.40000000000000002, 0.79999999999999993]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.30000000000000004, -0.30000000000000004, 0.89999999999999991]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.40000000000000002, -0.40000000000000002, 0.89999999999999991]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.40000000000000002, -0.40000000000000002, 0.89999999999999991]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.40000000000000002, -0.40000000000000002, 0.89999999999999991]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.40000000000000002, -0.40000000000000002, 0.89999999999999991]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.5, -0.5, 0.89999999999999991]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.5, -0.5, 0.89999999999999991]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.5, -0.5, 0.89999999999999991]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.40000000000000002, -0.40000000000000002, 0.99999999999999989]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.5, -0.5, 0.99999999999999989]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.5, -0.5, 0.99999999999999989]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.5, -0.5, 0.99999999999999989]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.40000000000000002, -0.40000000000000002, 1.0999999999999999]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.5, -0.5, 1.0999999999999999]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.5, -0.5, 1.0999999999999999]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.5, -0.5, 1.0999999999999999]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.5, -0.5, 1.0999999999999999]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.59999999999999998, -0.59999999999999998, 1.0999999999999999]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.59999999999999998, -0.59999999999999998, 1.0999999999999999]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.59999999999999998, -0.59999999999999998, 1.0999999999999999]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.5, -0.5, 1.2]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.59999999999999998, -0.59999999999999998, 1.2]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.59999999999999998, -0.59999999999999998, 1.2]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.59999999999999998, -0.59999999999999998, 1.2]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.5, -0.5, 1.3]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.59999999999999998, -0.59999999999999998, 1.3]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.59999999999999998, -0.59999999999999998, 1.3]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.59999999999999998, -0.59999999999999998, 1.3]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.59999999999999998, -0.59999999999999998, 1.3]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.69999999999999996, -0.69999999999999996, 1.3]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.69999999999999996, -0.69999999999999996, 1.3]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.69999999999999996, -0.69999999999999996, 1.3]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.59999999999999998, -0.59999999999999998, 1.4000000000000001]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.69999999999999996, -0.69999999999999996, 1.4000000000000001]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.69999999999999996, -0.69999999999999996, 1.4000000000000001]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.69999999999999996, -0.69999999999999996, 1.4000000000000001]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.69999999999999996, -0.69999999999999996, 1.4000000000000001]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.79999999999999993, -0.79999999999999993, 1.4000000000000001]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.79999999999999993, -0.79999999999999993, 1.4000000000000001]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.79999999999999993, -0.79999999999999993, 1.4000000000000001]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.69999999999999996, -0.69999999999999996, 1.5000000000000002]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.79999999999999993, -0.79999999999999993, 1.5000000000000002]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.79999999999999993, -0.79999999999999993, 1.5000000000000002]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.79999999999999993, -0.79999999999999993, 1.5000000000000002]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.69999999999999996, -0.69999999999999996, 1.6000000000000003]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.79999999999999993, -0.79999999999999993, 1.6000000000000003]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.79999999999999993, -0.79999999999999993, 1.6000000000000003]\n",
      "in_vec: [1, 1, 1]\n",
      "weights: [-0.79999999999999993, -0.79999999999999993, 1.6000000000000003]\n",
      "in_vec: [1, 1, 0]\n",
      "weights: [-0.79999999999999993, -0.79999999999999993, 1.6000000000000003]\n",
      "in_vec: [1, 0, 1]\n",
      "weights: [-0.89999999999999991, -0.89999999999999991, 1.6000000000000003]\n",
      "in_vec: [1, 0, 0]\n",
      "weights: [-0.89999999999999991, -0.89999999999999991, 1.6000000000000003]\n",
      "[-0.89999999999999991, -0.89999999999999991, 1.6000000000000003]\n"
     ]
    }
   ],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, ni, bias = False, weights = None, func = heaviside):\n",
    "        \"\"\"\n",
    "        Create a perceptron instance.\n",
    "        \"\"\"\n",
    "        # Number of inputs\n",
    "        self.ni = ni\n",
    "        \n",
    "        # Does the input vector contain a term for the bias (x_0) ?\n",
    "        self.bias = bias\n",
    "        \n",
    "        if weights:\n",
    "            self.weights = list(weights)\n",
    "            if bias:\n",
    "                assert(ni == len(weights))\n",
    "            else:\n",
    "                assert(ni+1 == len(weights))\n",
    "        else:\n",
    "            self.weights = [0]*ni\n",
    "            if not self.bias:\n",
    "                self.weights += [0]\n",
    "            \n",
    "        self.func = func\n",
    "            \n",
    "    def activate(self, in_vec):\n",
    "        in_vec = list(in_vec)\n",
    "        if not self.bias:\n",
    "            in_vec = [1]+in_vec\n",
    "        #assert(len(in_vec)==len(self.weights))\n",
    "        return self.func(sum([i*w for i, w in zip(in_vec, self.weights)]))\n",
    "    \n",
    "    def train(self, Xtr, alpha = 0.1, iterations = 100):\n",
    "        \"\"\"\n",
    "        Train the perceptron using the algorithm found on this page:\n",
    "        https://en.wikipedia.org/wiki/Perceptron#Steps\n",
    "        \n",
    "        Xtr : numpy.array\n",
    "          A two dimensional array.\n",
    "        alpha : float\n",
    "          The learning rate of the algorithm: how greedily the algorithm learns from the data.\n",
    "        iterations : int\n",
    "          Number of iterations of the algorithm.\n",
    "        \"\"\"\n",
    "        # Assert size of Xtr is correct\n",
    "        #if self.bias:\n",
    "        #    assert(Xtr.shape[1] == len(self.weights))\n",
    "        #else:\n",
    "        #    assert(Xtr.shape[1] == len(self.weights))\n",
    "            \n",
    "        for i in range(iterations):\n",
    "            j = i%Xtr.shape[0]\n",
    "            \n",
    "            # Get output\n",
    "            in_vec = [1] + list(Xtr[j, :-1])\n",
    "            print(\"in_vec:\", in_vec)\n",
    "            print(\"weights:\", self.weights)\n",
    "            target = Xtr[j, -1]\n",
    "            out = self.activate(in_vec)\n",
    "            # Update weights:\n",
    "            #for i, wi in enumerate(w):\n",
    "            #    w[i] = wi + alpha*(row[-1]- out)*row[i]\n",
    "            self.weights = [w + alpha * (target - out) * v for v, w in zip(in_vec, self.weights)]\n",
    "            \n",
    "        # have a default alpha size\n",
    "        # have a default number of iterations\n",
    "        # perhaps have some arguments about other termination criteria.\n",
    "print(Dand)\n",
    "        \n",
    "pec = Perceptron(2)\n",
    "pec.train(Dand)\n",
    "print(pec.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [-0.89999999999999991, -0.89999999999999991, 1.6000000000000003]\n",
      "\n",
      "Features and targets:\n",
      " [[1 1 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]] \n",
      "\n",
      "Out is 0 target is 1\n",
      "Out is 0 target is 0\n",
      "Out is 1 target is 0\n",
      "Out is 0 target is 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights:\\n\", pec.weights)\n",
    "print(\"\\nFeatures and targets:\\n\",Dand,\"\\n\")\n",
    "\n",
    "for i in range(Dand.shape[0]):\n",
    "    out = pec.activate(Dand[i,:2])\n",
    "    print(\"Out is \" + str(out) + \" target is \" + str (Dand[i, -1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, try and refactor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "[-0.20000000000000001, 0.10000000000000001, 0.20000000000000001]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, ni, weights = None, func = heaviside):\n",
    "        \"\"\"\n",
    "        Create a perceptron instance.\n",
    "        \"\"\"\n",
    "        # Number of inputs\n",
    "        self.ni = ni\n",
    "        \n",
    "        try:\n",
    "            self.weights = list(weights)\n",
    "            assert(len(self.weights)==ni+1)\n",
    "        except TypeError:\n",
    "            self.weights = [0]*(ni+1)\n",
    "            \n",
    "        self.func = func\n",
    "            \n",
    "    def activate(self, in_vec):\n",
    "        in_vec = [1]+list(in_vec)\n",
    "        assert(len(in_vec)==len(self.weights))\n",
    "        return self.func(sum([v*w for v, w in zip(in_vec, self.weights)]))\n",
    "    \n",
    "    def train(self, Xtr, alpha = 0.1, iterations = 100, verbose = False):\n",
    "        \"\"\"\n",
    "        Train the perceptron using the algorithm found on this page:\n",
    "        https://en.wikipedia.org/wiki/Perceptron#Steps\n",
    "        \n",
    "        Xtr : numpy.array\n",
    "          A two dimensional array.\n",
    "        alpha : float\n",
    "          The learning rate of the algorithm: how greedily the algorithm learns from the data.\n",
    "        iterations : int\n",
    "          Number of iterations of the algorithm.\n",
    "        \"\"\"\n",
    "        # Assert size of Xtr is correct\n",
    "        #if self.bias:\n",
    "        #    assert(Xtr.shape[1] == len(self.weights))\n",
    "        #else:\n",
    "        #    assert(Xtr.shape[1] == len(self.weights))\n",
    "            \n",
    "        for i in range(iterations):\n",
    "            j = i%Xtr.shape[0]\n",
    "            \n",
    "            # Get output\n",
    "            in_vec = list(Xtr[j, :-1])\n",
    "            if verbose:\n",
    "                print(\"in_vec:\", in_vec)\n",
    "                print(\"weights:\", self.weights)\n",
    "            target = Xtr[j, -1]\n",
    "            out = self.activate(in_vec)\n",
    "            # Update weights:\n",
    "            #for i, wi in enumerate(w):\n",
    "            #    w[i] = wi + alpha*(row[-1]- out)*row[i]\n",
    "            self.weights = [w + alpha * (target - out) * v for v, w in zip([1]+in_vec, self.weights)]\n",
    "            \n",
    "        # have a default alpha size\n",
    "        # have a default number of iterations\n",
    "        # perhaps have some arguments about other termination criteria.\n",
    "    def accuracy(self, Xtr):\n",
    "        results = [self.activate(row[:-1])==row[-1] for row in Xtr]\n",
    "        return sum(results)/len(results)\n",
    "    \n",
    "print(Dand)\n",
    "        \n",
    "pec = Perceptron(2)\n",
    "pec.train(Dand)\n",
    "print(pec.weights)\n",
    "for row in Dand:\n",
    "    assert(pec.activate(row[:-1])==row[-1])\n",
    "    \n",
    "print(pec.accuracy(Dand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! That seems to work and I took out that silliness of bias. I'd like to return to that though, because ideally when passing vectors around internally, I'd like to not have to worry about ```[1]+in_vec``` vs ```in_vec```\n",
    "\n",
    "### Perceptron conclusions\n",
    "\n",
    "I could spend more time playing with perceptrons, but I think that'll do for now. I might create an ANN package and solidify what I have here with a test suite. This is the end of part one, though, it's time to return to learning about ANNs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
