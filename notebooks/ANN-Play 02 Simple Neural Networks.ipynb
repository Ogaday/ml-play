{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network Play 2: Simple Neural Networks\n",
    "\n",
    "Part one can be found [here](http://nbviewer.ipython.org/github/Ogaday/ml-play/blob/master/ANN-Play%2001%20Perceptrons.ipynb) and is part of a series on github [here](https://github.com/Ogaday/ml-play). Sources are inlcuded in the first notebook.\n",
    "\n",
    "This chapter will be focussed on building and activating simple neural networks.\n",
    "\n",
    "## A simple neural network\n",
    "\n",
    "Neural networks can be characterized graphically: each neuron or node is represented by a vertex and each axon by an edge. There are several strategies for defining graphs. Commonly, graphs are defined by a triple: $(V, E, \\varepsilon)$ where $V$ is the set of vertices, $E$ is the set of edges and $\\varepsilon$ is the endpoint map $\\varepsilon:E\\to P_1\\{V\\}\\cup P_2\\{V\\}$. (A mapping of edges to the union of the power set of one and the power set of two of the set of vertices). In this case, the graph is weighted, so we introduce a metric map $\\mu$ such that $\\mu: E \\to \\mathbb R$, where $\\mathbb R$ is the set of real values and the mapping denotes of the weight on each edge. Furthermore, this graph should be loop free, so in fact we can simplify $\\varepsilon$ to $\\varepsilon:E\\to P_2\\{V\\}$ and as this graph is directed, order of the elements of $p_i \\in P_2\\{V\\}$ matters.\n",
    "\n",
    "However, there is a representation that is better suited for translation to a data structure:  the adjacency matrix. For a weighted graph, this is simply defined by $A_{ij} = w_{ij}$ where $w_{ij}$ is the weight of the edge between the vertices $i$ and $j$. This is a much better way of representing a graph digitally.\n",
    "\n",
    "$\\begin{bmatrix}a & b\\\\c & d\\end{bmatrix}$\n",
    "\n",
    "Let's try to define the architecture necessary to describe the exercise in Carlos Gershenson's [document](https://datajobs.com/data-science-repo/Neural-Net-%5BCarlos-Gershenson%5D.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-66b79c2a63a1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-66b79c2a63a1>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    from \"..\" import ann\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from \"..\" import ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = set(range(6))\n",
    "E = {(0,1),(0,2),(1,3),(1,4),(2,3),(2,4),(3,5),(4,5)}\n",
    "weights = {e:0 for for e in E}\n",
    "N = Network(V, E, weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
