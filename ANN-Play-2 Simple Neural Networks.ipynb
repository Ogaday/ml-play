{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network Play 2: Simple Neural Networks\n",
    "\n",
    "Part one can be found [here](http://nbviewer.ipython.org/github/Ogaday/ml-play/blob/master/ANN-Play%2001%20Perceptrons.ipynb) and is part of a series on github [here](https://github.com/Ogaday/ml-play). Sources are inlcuded in the first notebook.\n",
    "\n",
    "This chapter will be focussed on building and activating simple neural networks.\n",
    "\n",
    "## A simple neural network\n",
    "\n",
    "I will try a graphical representation of the neural network. A graph is often defined by a triple: $(V, E, \\varepsilon)$ where $V$ is the set of vertices, $E$ is the set of edges and $\\varepsilon$ is the endpoint map $\\varepsilon:E\\to P_1\\{V\\}\\cup P_2\\{V\\}$. (A mapping of edges to the union of the power set of one and the power set of two of the set of vertices).\n",
    "\n",
    "In this case, the graph is weighted, so we introduce a metric map $\\mu$ such that $\\mu: E \\to \\mathbb R$, where $\\mathbb R$ is the set of real values and the mapping denotes of the weight on each edge. Furthermore, this graph should be loop free, so in fact we can simplify $\\varepsilon$ to $\\varepsilon:E\\to P_2\\{V\\}$ and as this graph is directed, order of the elements of $p_i \\in P_2\\{V\\}$ matters.\n",
    "\n",
    "Let's try to define the architecture necessary to describe the exercise in Carlos Gershenson's [document](https://datajobs.com/data-science-repo/Neural-Net-%5BCarlos-Gershenson%5D.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, V, E, weights, func = None):\n",
    "        \"\"\"\n",
    "        The conditions are:\n",
    "         * No cycles.\n",
    "         * Every vertex must be mapped to by at least one edge (ie. fully connected).\n",
    "         * V must be orderable. min(V) will be set to the abstract source node from which inputs come and the\n",
    "           max(V) will be set to the sink node which will collect the outputs. Therefore min(V) must not be the tail\n",
    "           of any edge mapped tuple, and similarly max(V) must not be the head of any edge mapped tuple.\n",
    "        \"\"\"\n",
    "        self.V = V\n",
    "        self.E = E\n",
    "        self.network = {e:0 for e in E}    # activated values on the edges. One approach for now.\n",
    "        self.weights = weights\n",
    "        if func:\n",
    "            self.func = func\n",
    "        else:\n",
    "            self.func = lambda x: pass\n",
    "    \n",
    "    def activate(self, in_vec):\n",
    "        \"\"\"\n",
    "        Fire the neurons! With vector \"in_vec\", fire the neurons and return and \"out_vec\".\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fire(self, v, in_vec = None):\n",
    "        \"\"\"\n",
    "        Maybe one day I'll make a vertex class that clear this up sometime. For now though, take a vertex, fire it\n",
    "        based upon input and update the network property of Network.\n",
    "        \n",
    "        For the source and sink vertices, ie. the input and output vectors, they are sorted by the rank of the\n",
    "        connected vertices.\n",
    "        \n",
    "        RE: vertex class, can be subclassed for source and sink this fire method could be overwritten...\n",
    "        \"\"\"\n",
    "        ins = {e for e in self.E if e[1] == v}\n",
    "        outs = {e for e in self.E if e[0] == v}\n",
    "        \n",
    "        if ins == {}:\n",
    "            # Then we are the source vertex\n",
    "            sorted_outs = [(v, o) for o in [e[1] for e in outs].sort()]    # Important that we consider order here.\n",
    "            if in_vec:\n",
    "                for e, i in zip(sorted_outs, in_vec):\n",
    "                    self.network[e] = i\n",
    "            else:\n",
    "                raise Exception(\"Input vector not defined.\")\n",
    "        elif outs == {}:\n",
    "            # Then we are the sink vertex\n",
    "            # Don't need to do anything?\n",
    "            pass\n",
    "        else:\n",
    "            # The we are neither a source nor sink vertex and so we are a vertex in the neural network to be fired.\n",
    "            val = sum([self.weights[e]*self.network[e] for e in ins])\n",
    "            for e in outs:\n",
    "                self.network[e] = val\n",
    "            \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = set(range(6))\n",
    "E = {(0,1),(0,2),(1,3),(1,4),(2,3),(2,4),(3,5),(4,5)}\n",
    "weights = {e:0 for for e in E}\n",
    "N = Network(V, E, weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
